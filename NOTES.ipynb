{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Ultimate 2024 - Bert Gollnick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Course Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deep Learning Introduction\n",
    "\n",
    "Layer Types:\n",
    "- Dense Layer - all perceptrons have a connection between one another\n",
    "- Convolutional Layer - layers consist of \"filters\", not all perceptrons connected\n",
    "- Recurrent Neural Networks - take their own output as an input with delay based on context\n",
    "- Long Short-Term Memory - uses a 'memory cell' for temporal sequences\n",
    "\n",
    "Activation Functions:\n",
    "- ReLU\n",
    "    - LeakyReLU: x for x => 0, x * a for x < 0, a is usually .01\n",
    "        - this ensures the gradient is never 0\n",
    "- tanh - nonlinear, but has a small range (*normalize*), activation btwn -1, 1\n",
    "- sigmoid - nonlinear, activation btwn 0, 1 -> better for probability\n",
    "- softmax - probability among n classes, used for multi-class classification\n",
    "\n",
    "Loss Functions:\n",
    "- Regression\n",
    "    - Mean Squared Error\n",
    "    - Mean Absolute Error - MSE w/ abs instead of square\n",
    "    - Mean Bias Error - take away the abs sign now\n",
    "    - Output layer must have 1 node, typically used with linear activation functions\n",
    "- Binary Classification\n",
    "    - Binary Cross Entropy\n",
    "    - Hinge (SVM) Loss\n",
    "    - Output layer must have 1 node, typically used with sigmoid activation\n",
    "- Multi-label Classification\n",
    "    - Multi-label Cross Entropy\n",
    "    - Output layer has n nodes, typical activation function is softmax\n",
    "\n",
    "Optimizers:\n",
    "- Gradient Descent\n",
    "    - Learning rate: can be too large (misses min) and too small (takes too long)\n",
    "- Adagrad - adapts learning rate to features, works well for sparse data sets\n",
    "- Adam - ADAptive Momentum estimation, includes previous gradients in calculation, popular\n",
    "- Stochastic Gradient Descent, Batch Gradient Descent\n",
    "\n",
    "Frameworks:\n",
    "- Tensorflow - most popular, made by google\n",
    "    - he's making it seem like we're using tensorflow -_-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Evaluation\n",
    "- High Bias = Low Accuracy, High Variance = Low Precision\n",
    "    - High Bias means R^2 values of training or validation are off\n",
    "    - High Variance means the difference between the R^2 values of training and validation is high\n",
    "- General rule: More **complex models** -> Lower Bias and More Variance\n",
    "- Low variance algorithms: Linear Regression, LDA, Logistic Regression\n",
    "- High variance algorithms: Decision Trees, kNN, SVM\n",
    "- <img src=\"tttgraph.png\" width=\"300\" height=\"260\" alt=\"train-test trend graph\"> <img src=\"bvtgraph.png\" width=\"300\" height=\"260\" alt=\"bias-variance graph\">\n",
    "- Resampling: e.g. train 5 models using 80/20 train/test splits so that all data is used for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Neural Network from Scratch\n",
    "- working on files 015_NeuralNetworkFromScratch/*\n",
    "- StandardScaler from sklearn.preprocessing to normalize\n",
    "    - X_train_scale = scaler.fit_transform(X_train)\n",
    "    - X_test_scale = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Tensors\n",
    "- gradients are calculated automatically\n",
    "- working on file 020_TensorIntro/Tensors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# create tensor with gradients enabled\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "# create second tensor depending on first tensor\n",
    "y = (x - 3) * (x - 6) * (x - 4)\n",
    "# calculate gradients\n",
    "y.backward() # this populates the grad of the x tensor\n",
    "# show gradient of first tensor\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. PyTorch Modeling Introduction\n",
    "working on files 030_ModelingIntroduction/*\n",
    "\n",
    "- 00 - linear regression from scratch\n",
    "- 10 - linear regression with model class\n",
    "     - more epochs: takes longer to train, better model, higher chance of instability\n",
    "- 20 - passing data as batch is literally just a slice from the tensor\n",
    "     - small batch size:\n",
    "        - less gpu usage, more iterations, less training stability\n",
    "     - bigger batch sizes are the opposite\n",
    "- 30 - `from torch.utils.data import Dataset, DataLoader`\n",
    "- 40 - model saving/loading `torch.save() and torch.load()`\n",
    "     - state dictionary .pth\n",
    "- 50 - hyperparameter tuning\n",
    "     - packages: RayTune, Optuna, skorch\n",
    "     - hyperparams:\n",
    "        - topology: number of nodes, layer types, activation functions\n",
    "            - more hidden layers/nodes per layer: can learn more complex patterns\n",
    "            - less hidden layers/nodes per layer: less training time, more inference time, less risk of overfitting\n",
    "        - objects: loss function, optimizer\n",
    "        - training: learning rate, batch size, number of epochs\n",
    "     - types of hyperparam tuning: grid (test all combinations of guesses), random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models are usually separate objects from the optimizer and loss functions\n",
    "# but you can define them in the model and it simplifies the training loop:\n",
    "#\n",
    "# ngl i think the step function shown in the training loop may have to be\n",
    "# implemented manually\n",
    "epochs = 1000\n",
    "data_loader = [([1,2], [2, 3]), ([3, 4], [4, 5])]\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(128, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 10),\n",
    "    torch.nn.LogSoftmax(),\n",
    "    # loss=torch.nn.NLLLoss(),\n",
    "    # optimizer=torch.optim.Adam(lr=.01)\n",
    ")\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     for i, (feature, label) in enumerate(data_loader):\n",
    "#         model.step(feature, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Classification\n",
    "- confusion matrix\n",
    "- ROC Curve - FPR v. TPR\n",
    "- all work done in folder 045"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
