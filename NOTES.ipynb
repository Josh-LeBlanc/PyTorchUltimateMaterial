{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Ultimate 2024 - Bert Gollnick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Course Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deep Learning Introduction\n",
    "\n",
    "Layer Types:\n",
    "- Dense Layer - all perceptrons have a connection between one another\n",
    "- Convolutional Layer - layers consist of \"filters\", not all perceptrons connected\n",
    "- Recurrent Neural Networks - take their own output as an input with delay based on context\n",
    "- Long Short-Term Memory - uses a 'memory cell' for temporal sequences\n",
    "\n",
    "Activation Functions:\n",
    "- ReLU\n",
    "    - LeakyReLU: x for x => 0, x * a for x < 0, a is usually .01\n",
    "        - this ensures the gradient is never 0\n",
    "- tanh - nonlinear, but has a small range (*normalize*), activation btwn -1, 1\n",
    "- sigmoid - nonlinear, activation btwn 0, 1 -> better for probability\n",
    "- softmax - probability among n classes, used for multi-class classification\n",
    "\n",
    "Loss Functions:\n",
    "- Regression\n",
    "    - Mean Squared Error\n",
    "    - Mean Absolute Error - MSE w/ abs instead of square\n",
    "    - Mean Bias Error - take away the abs sign now\n",
    "    - Output layer must have 1 node, typically used with linear activation functions\n",
    "- Binary Classification\n",
    "    - Binary Cross Entropy\n",
    "    - Hinge (SVM) Loss\n",
    "    - Output layer must have 1 node, typically used with sigmoid activation\n",
    "- Multi-label Classification\n",
    "    - Multi-label Cross Entropy\n",
    "    - Output layer has n nodes, typical activation function is softmax\n",
    "\n",
    "Optimizers:\n",
    "- Gradient Descent\n",
    "    - Learning rate: can be too large (misses min) and too small (takes too long)\n",
    "- Adagrad - adapts learning rate to features, works well for sparse data sets\n",
    "- Adam - ADAptive Momentum estimation, includes previous gradients in calculation, popular\n",
    "- Stochastic Gradient Descent, Batch Gradient Descent\n",
    "\n",
    "Frameworks:\n",
    "- Tensorflow - most popular, made by google\n",
    "    - he's making it seem like we're using tensorflow -_-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Evaluation\n",
    "- High Bias = Low Accuracy, High Variance = Low Precision\n",
    "    - High Bias means R^2 values of training or validation are off\n",
    "    - High Variance means the difference between the R^2 values of training and validation is high\n",
    "- General rule: More **complex models** -> Lower Bias and More Variance\n",
    "- Low variance algorithms: Linear Regression, LDA, Logistic Regression\n",
    "- High variance algorithms: Decision Trees, kNN, SVM\n",
    "- <img src=\"tttgraph.png\" width=\"300\" height=\"260\" alt=\"train-test trend graph\"> <img src=\"bvtgraph.png\" width=\"300\" height=\"260\" alt=\"bias-variance graph\">\n",
    "- Resampling: e.g. train 5 models using 80/20 train/test splits so that all data is used for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Neural Network from Scratch\n",
    "- working on files 015_NeuralNetworkFromScratch/*\n",
    "- StandardScaler from sklearn.preprocessing to normalize\n",
    "    - X_train_scale = scaler.fit_transform(X_train)\n",
    "    - X_test_scale = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Tensors\n",
    "- gradients are calculated automatically\n",
    "- working on file 020_TensorIntro/Tensors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(31.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# create tensor with gradients enabled\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "# create second tensor depending on first tensor\n",
    "y = (x - 3) * (x - 6) * (x - 4)\n",
    "# calculate gradients\n",
    "y.backward() # this populates the grad of the x tensor\n",
    "# show gradient of first tensor\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. PyTorch Modeling Introduction\n",
    "working on files 030_ModelingIntroduction/*\n",
    "\n",
    "- 00 - linear regression from scratch\n",
    "- 10 - linear regression with model class\n",
    "     - more epochs: takes longer to train, better model, higher chance of instability\n",
    "- 20 - passing data as batch is literally just a slice from the tensor\n",
    "     - small batch size:\n",
    "        - less gpu usage, more iterations, less training stability\n",
    "     - bigger batch sizes are the opposite\n",
    "- 30 - `from torch.utils.data import Dataset, DataLoader`\n",
    "- 40 - model saving/loading `torch.save() and torch.load()`\n",
    "     - state dictionary .pth\n",
    "- 50 - hyperparameter tuning\n",
    "     - packages: RayTune, Optuna, skorch\n",
    "     - hyperparams:\n",
    "        - topology: number of nodes, layer types, activation functions\n",
    "            - more hidden layers/nodes per layer: can learn more complex patterns\n",
    "            - less hidden layers/nodes per layer: less training time, more inference time, less risk of overfitting\n",
    "        - objects: loss function, optimizer\n",
    "        - training: learning rate, batch size, number of epochs\n",
    "     - types of hyperparam tuning: grid (test all combinations of guesses), random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models are usually separate objects from the optimizer and loss functions\n",
    "# but you can define them in the model and it simplifies the training loop:\n",
    "#\n",
    "# ngl i think the step function shown in the training loop may have to be\n",
    "# implemented manually\n",
    "epochs = 1000\n",
    "data_loader = [([1,2], [2, 3]), ([3, 4], [4, 5])]\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(128, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 10),\n",
    "    torch.nn.LogSoftmax(),\n",
    "    # loss=torch.nn.NLLLoss(),\n",
    "    # optimizer=torch.optim.Adam(lr=.01)\n",
    ")\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     for i, (feature, label) in enumerate(data_loader):\n",
    "#         model.step(feature, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Classification\n",
    "- confusion matrix\n",
    "- ROC Curve - FPR v. TPR\n",
    "- all work done in folder 045"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. CNN: Image Classification\n",
    "- applying a convolutional filter to an image results in a **convolutional layer**\n",
    "- after convolutional layer is **max pooling** which takes the max value from the given areas\n",
    "    - depending on convolutional layer, average pooling or min pooling may be better\n",
    "- example network: conv, max pooling, conv, max pooling, dense, softmax\n",
    "    - typically increasing feature map counts and decreasing size of feature maps\n",
    "- pros: computer vision, high quality predictions\n",
    "- cons: many params, requires lots of experience, computationally expensive\n",
    "- **image preprocessing**:\n",
    "    - scaling down image size\n",
    "    - center crop\n",
    "    - grayscale\n",
    "    - random rotation\n",
    "    - random vertical flip\n",
    "    - toTensor - normalize\n",
    "    - transforms.Compose performs several image preprocessing steps at once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. CNN: Audio Classification\n",
    "working in folder 065\n",
    "- literally just turned the audio data into an image and then did a similar multiclass CNN to ch 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. CNN: Object Detection\n",
    "\n",
    "- very computationally expensive to train\n",
    "- many algorithms: fast R-CNN, faster R-CNN, YOLO (you only look once), SSD (single shot detector)\n",
    "- **task**: object detection with detecto, working in folder 070\n",
    "    - downloaded a faster-rcnn - `model.get_internal_model()` shows transforms, layers\n",
    "- **yolo**: one-stage object detection\n",
    "    - based on yolo papers\n",
    "    - image is divided into grids\n",
    "    - simultaneously predicts bounding box w/ confidence and class probability of grid\n",
    "    - fairly fast, but also inaccurate\n",
    "    - train: `python train.py --weights yolov7-e6e.pt --data \"data/masks.yaml\" --workers 1 --batch-size 4 --img 640 --cfg cfg/training/yolov7-masks.yaml --name yolov7 --epochs 50`\n",
    "    - test: `python detect.py --weights runs/train/yolov73/weights/best.pt --conf 0.4 --img-size 640 --source ./test/images/file_to_test.png` -- also works with videos\n",
    "- **labeling formats:**\n",
    "    - Pascal VOC: 20 classes, 3000 images, XML labelling, one file per element, TL(x,y), BR(x,y)\n",
    "    - COCO: 92 classes, 2.5M images, JSON labelling, one file per datasetl, TL(x), TL(y), width, height\n",
    "    - YOLO: data dependent, TXT labelling, one file per element, Center(x), Center(y), width, height\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Style Transfer\n",
    "\n",
    "**paper: \"A Neural Algorithm of Artistic Style\"** - Gatys, Ecker, Bethge <a href=\"https://arxiv.org/pdf/1508.06576.pdf\">paper</a>\n",
    "\n",
    "- done with pretrained networks, e.g. VGG16, VGG19\n",
    "    - VGG19 has 16 convolutional layers, 5 pooling layers\n",
    "- CNN does not change during training\n",
    "- just target image modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Pretrained Models\n",
    "going to be using pretrained model DenseNet12, <a href=\"https://arxiv.org/pdf/1608.06993.pdf\">paper</a>\n",
    "\n",
    "**transfer learning** is when you further train a pretrained model, that way it doesn't have to start from completely randomized weights\n",
    "- it is usually the convolutional layers that are already trained, and we train the dense layers and output labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Recurrent Neural Networks\n",
    "- useful for sequential data, where the previous data points are relevant to the current data point\n",
    "- different architectures: <br>\n",
    "<img src=\"rnn-archs.png\" width=\"550\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
